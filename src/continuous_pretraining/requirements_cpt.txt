# 继续预训练依赖包
torch>=2.0.0
transformers>=4.35.0
datasets>=2.14.0
tqdm>=4.64.0
numpy>=1.21.0

# Muon优化器 (如果可用)
# 注意: Muon优化器可能需要从特定源安装
# pip install muon-optimizer

# Flash Attention (可选，用于提升注意力计算效率)
# 注意: Flash Attention需要特定的CUDA版本和PyTorch版本
# pip install flash-attn --no-build-isolation

# 其他可选依赖
accelerate>=0.20.0
deepspeed>=0.9.0  # 用于分布式训练
wandb>=0.15.0     # 用于实验跟踪
tensorboard>=2.10.0  # 用于可视化
