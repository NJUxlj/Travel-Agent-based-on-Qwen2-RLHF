# Corrective Retrieval Augmented Generation (CRAG)

这篇论文提出了**Corrective Retrieval Augmented Generation (CRAG)**，旨在解决传统检索增强生成（RAG）对检索结果过度依赖的问题，通过动态评估检索质量并触发不同策略来提升生成鲁棒性。以下是方法的详细解释：

---

### **1. 核心问题**
传统RAG的生成质量高度依赖检索文档的相关性。若检索结果不相关，生成内容可能包含错误或幻觉（hallucination）。CRAG的核心目标是：
- **动态评估检索质量**：判断检索结果是否可靠。
- **自适应纠正策略**：根据评估结果触发不同知识利用方式。
- **优化知识利用**：过滤噪声，补充外部知识。

---

### **2. 方法框架**
CRAG包含三个关键模块：

#### **（1）检索评估器（Retrieval Evaluator）**
- **功能**：评估检索文档与查询的相关性，输出置信度分数。
- **实现**：
  - 基于T5-large模型微调，轻量级（0.77B参数）。
  - 输入为查询-文档对，输出相关性分数（-1到1）。
  - 对比实验显示，其准确性显著优于直接使用ChatGPT评估。

#### **（2）动作触发机制（Action Trigger）**
根据置信度分数触发三类动作：
- **Correct（正确）**：至少一个文档分数高于阈值（如0.59）。
  - **操作**：对文档进行知识精炼（分解-过滤-重组）。
- **Incorrect（不正确）**：所有文档分数低于下限（如-0.99）。
  - **操作**：丢弃检索结果，启动网络搜索补充外部知识。
- **Ambiguous（模糊）**：介于两者之间。
  - **操作**：结合精炼后的内部文档和外部搜索结果。

#### **（3）知识优化策略**
- **知识精炼（Knowledge Refinement）**：
  - **分解**：将文档分割为细粒度知识片段（如句子级）。
  - **过滤**：用评估器筛选高相关性片段。
  - **重组**：保留关键片段，拼接为内部知识（`k_in`）。
- **网络搜索（Web Search）**：
  - **查询改写**：用ChatGPT将输入问题重写为搜索关键词（如“Death of a Batman; screenwriter; Wikipedia”）。
  - **搜索与筛选**：通过API（如Google Search）获取网页内容，提取相关段落作为外部知识（`k_ex`）。

---

### **3. 技术亮点**
- **动态阈值调整**：根据任务设置不同阈值（如PopQA为0.59/-0.99），灵活应对不同场景。
- **插件式设计**：可无缝集成到现有RAG框架（如Self-RAG），无需修改生成模型。
- **混合知识源**：结合静态文档库（如Wikipedia）和动态网络搜索，扩展知识覆盖范围。

---

### **4. 实验结果**
在四个数据集上的表现：
- **PopQA（短文本生成）**：CRAG比RAG提升19%准确率。
- **Biography（长文本生成）**：FactScore提升14.9%。
- **PubHealth（判断题）**：准确率提升36.6%。
- **Arc-Challenge（选择题）**：准确率提升8.1%。

关键结论：
- **鲁棒性提升**：即使检索质量下降，CRAG的生成性能下降幅度小于传统方法（见图3）。
- **通用性**：适用于不同生成任务（短/长文本、封闭式问题）。

---

### **5. 局限与未来方向**
- **依赖评估器微调**：需针对不同领域调整阈值和训练数据。
- **网络搜索偏差**：可能引入低质量或噪声信息。
- **扩展性**：未来可探索更稳定的知识融合策略。

---

CRAG通过动态评估、自适应策略和知识优化，显著提升了RAG在低质量检索场景下的鲁棒性，为生成模型的可靠性提供了新思路。













---





# Self-RAG

SELF-RAG（自省式检索增强生成）是一种创新框架，旨在通过动态检索和自省机制提升大语言模型的事实性和生成质量。以下是其核心原理和实现细节的详细解析：

---

### **核心思想**
传统RAG方法固定检索段落数量，可能引入无关信息或抑制模型创造力。SELF-RAG通过**自省标记（Reflection Tokens）**实现动态控制：
- **按需检索**：模型自主判断何时需要外部知识，避免不必要的检索。
- **多维度评估**：对检索内容和生成文本进行实时质量评估，确保信息相关性和事实准确性。
- **可控生成**：通过反射标记的权重调整，灵活平衡事实严谨性与创造性。

---

### **方法架构**
#### 1. **反射标记类型**
- **检索标记（Retrieve）**：决定是否触发检索（Yes/No/Continue）。
- **批判标记（Critique）**：
  - **相关性（ISREL）**：段落是否与问题相关（Relevant/Irrelevant）。
  - **支持度（ISSUP）**：生成内容是否被段落支持（Fully/Partially/No support）。
  - **效用（ISUSE）**：回答的整体质量评分（1-5分）。

#### 2. **训练流程**
- **批评模型（Critic）**：基于GPT-4生成反射标记的监督数据，训练模型预测标记。
- **生成模型（Generator）**：在原始文本中插入反射标记，通过标准语言模型目标训练，使模型学会自主生成标记和文本。

#### 3. **推理机制**
- **动态检索**：根据`Retrieve`标记概率阈值决定是否检索。
- **并行处理**：对多个检索段落生成候选文本，通过反射标记评分（如`ISSUP`权重高则偏好事实性强的输出）。
- **段落级束搜索**：结合标记概率加权得分选择最优生成路径，支持硬性约束（如过滤未支持的生成）。

---

### **关键优势**
1. **事实性提升**：在PopQA等开放域QA任务中，SELF-RAG的准确率比ChatGPT提高近20%，引用精确度达70%以上。
2. **灵活控制**：通过调整反射标记权重，可定制模型行为（如提高事实严谨性时，`ISSUP`权重增加）。
3. **效率优化**：自适应检索减少冗余计算，相比固定检索策略推理速度提升30%。

---

### **示例说明**
**输入**：“加州的名字来源是什么？”
1. **检索判断**：模型生成`Retrieve=Yes`，触发检索。
2. **段落评估**：检索到段落A（提及加州源自西班牙小说），标记`ISREL=Relevant`。
3. **生成与批判**：输出“加州得名于西班牙小说中的岛屿”，并标记`ISSUP=Fully Supported`。
4. **效用评分**：最终回答获得`ISUSE=5`，表示信息完整且有用。

---

### **总结**
SELF-RAG通过将检索决策和内容评估内化为模型的自省能力，实现了生成质量与事实性的平衡。其模块化设计使其易于适配不同任务需求，为知识密集型应用提供了可靠解决方案。













# Memory Walker

MEMWALKER是一种基于交互式阅读的长文本处理方法，通过结构化内存树和动态导航机制突破语言模型的上下文限制。以下是其核心机制的详细解释：

### 一、方法框架
#### 1. 内存树构建（Memory Tree Construction）
- **文本分块**：将长文本分割为适合模型上下文窗口的小段（如1000-1200 tokens）。
- **层次化摘要**：
  - **叶子节点**：每段文本生成一级摘要节点（Level 1）。
  - **递归聚合**：每8-5个相邻节点生成更高层摘要（Level 2），重复该过程直至形成根节点。
  - **树结构特性**：深度与文本长度对数相关，根节点包含全局概要。

#### 2. 导航机制（Navigation）
- **交互式路径选择**：
  - **非叶节点**：模型比较子节点摘要，选择最相关路径（如示例中模型通过推理选择"Summary 0"）。
  - **叶子节点**：直接判断当前文本段是否能回答问题，否则回退父节点。
- **工作记忆**：保留导航路径中的关键信息（如父节点摘要），增强上下文连贯性。
- **错误恢复**：支持回退动作（revert action），15-20%的案例中触发回退，其中60-80%能成功纠正路径。

### 二、技术优势
1. **动态注意力分配**  
   相比传统全窗口注意力机制，MEMWALKER的token处理量减少30-40%（图4），通过聚焦相关段落显著降低计算开销。

2. **可解释性增强**  
   导航路径形成可视化推理链（如表1示例），明确展示"选择Summary 0→进入Segment 2→发现信息不足→回退→选择Segment 3"的决策过程。

3. **零样本适应性**  
   无需微调即可适配不同领域文本（小说、政府报告、剧本），依赖LLM的通用推理能力实现跨任务迁移。

### 三、实验验证
| 数据集       | 长文本准确率 | 相对基线提升 |
|--------------|--------------|--------------|
| QuALITY      | 73.6%        | +9.1%        |
| SummScreenFD | 64.5%        | +1.8%        | 
| GovReport    | 60.4%        | +8.3%        |

*基线对比对象：16k tokens的LongChat模型、检索增强方法*

### 四、局限与展望
1. **内存树构建成本**  
   超长文本（>100k tokens）可能生成过深树结构，未来可探索哈希树等混合数据结构。

2. **模型能力依赖**  
   70B参数模型才能有效执行导航（表3），较小模型表现显著下降，需研究蒸馏技术降低计算需求。

3. **应用扩展**  
   当前聚焦QA任务，未来可扩展至长文档摘要、法律条文分析等场景，并探索与RAG架构的结合。











---



# Read-Agent









---




# Seven Failure Points When Engineering a Retrieval Augmented Generation System





















# The Power of Noise

